#!/usr/bin/env python3

# Importing packages for programming, odds and ends
import argparse
import re
import itertools
import json
import time
import statistics
import sys
import gzip

# Importing packages for the heart of it, fuzzy regex and SeqIO classes
import regex
from Bio import SeqIO
from Bio import Seq, SeqRecord




def reader(input_file, is_gzipped, 
    output_file, failed_file, report_file,
    operations_array, filters , outputs_array,
    out_format,
    verbosity
    ):

    """
    Designed to be called later. This takes some input filepath, where 'None' 
    gets treated like the input is STDIN. 
    """

    ### Open up file handles

    # If that's none, which is default, then we're taking sequences by STDIN
    if input_file is None:
        input_seqs = SeqIO.parse(sys.stdin,"fastq")
    else:
        # Or if it's gzipped then it's from a gzipped file (but no gzipped
        # STDIN plz, just zcat it
        if is_gzipped:
            with gzip.open(input_file,"rt") as input_file_gz:
                input_seqs = SeqIO.parse(input_file_gz,"fastq")
        # Otherwise is a flat file I assume
        else:
            input_seqs = SeqIO.parse(input_file,"fastq")

    # Opening up output file handles, will hand them off to each chop 
    # If no output file, then I'm spitting it out on STDOUT
    if output_file is None:
        output = sys.stdout
    # If you've specified a file, then that's here
    else:
        output = open(output_file,"a")

    # If no failed file specified, then we're just ignoring it
    if failed_file is None:
        failed = None
    # But if specified, then it gets written
    else:
        failed = open(failed_file,"a")

    # Same for optional report
    if report_file is None:
        report = None
    else:
        report = open(report_file,"a")

    # Making a dummyspacer thing and seq_holder, this gets copied for each
    # chop function call, so it's where the internals can hold matches for
    # subsequent group searches. Dummy spacer is for odd formatting outputs.
    # Not very necessary now that I put it out as SAM...
    seq_holder = {
        'dummyspacer': SeqRecord.SeqRecord(Seq.Seq("X"),id="dummyspacer"),
        'input': None}
    seq_holder['dummyspacer'].letter_annotations['phred_quality'] = [40]

    ### Do the chop-ing

    for each_seq in input_seqs:

        chop(each_seq, # Each sequence, one by one...
            operations_array, filters, outputs_array, # Things todo!
            output, failed, report, # File handles
            seq_holder.copy(),  # Here we pass in the sequence holder, as it's
                                # pre-loaded with the dummyspacer
            out_format, # Er... what format for output?
            verbosity
            )

    return(0)


def chop(input_record, 
    operations_array, filters, outputs_array, 
    output, failed, report,
    seq_holder,
    out_format,
    verbosity
    ):
    """
    This one takes each record, applies the operations, evaluates the filters,
    generates outputs, and writes them to output handles as appropriate.
    """

    # We make a score holder (for statistics), put input_record in seq_holder
    # copy we have.
    scores_holder = dict()
    seq_holder['input'] = input_record

    # Chop grained verbosity
    if verbosity >= 2:
        print("\n["+str(time.time())+"] : starting to process : "+
            input_record.id+"\n  "+input_record.seq+"\n  "+ 
            str(input_record.letter_annotations['phred_quality']),
            file=sys.stderr)

    # This should fail if you didn't specify anything taking 
    # from input stream!
    assert operations_array[0][0] == "input", (
        "can't find the sequence named `input`, rather we see `"+
        operations_array[0][0]+"` in the holder, so breaking. You should "+
        "have the first operation start with `input` as a source." )

    for operation_name, operation in enumerate(operations_array):

        operation_name = str(operation_name)

        # Okay, the one required for input here, that needs to have actually
        # matched in order to generate this next one, so just continuing if
        # that's not there. Outputs should be made tolerate of missing groups.
        try: 
            seq_holder[operation[0]]
        except:
            continue

        if verbosity >= 3:
            print("\n["+str(time.time())+"] : attempting to match : "+
                str(operation[1])+" against "+seq_holder[operation[0]].seq,
                file=sys.stderr)

        # Here we execute the actual meat of the business
        fuzzy_match = operation[1].search(
                str(seq_holder[operation[0]].seq).upper() )

        if verbosity >= 3:
            print("\n["+str(time.time())+"] : match is : "+str(fuzzy_match),
                file=sys.stderr)

        # This is fine, just means the pattern couldn't match at all
        if fuzzy_match is None:
            continue
        # If we did match, then we store them in places
        else:
            # We use tuples to store all the details of the kinds
            # of errors that allowed the match, but we only need that for
            # the filters, so test first
            if filters is not None:
                (scores_holder[str(operation_name)+'_substitutions'],
                    scores_holder[operation_name+'_insertions'],
                    scores_holder[operation_name+'_deletions']
                    ) = fuzzy_match.fuzzy_counts
            # Then for each of the groups matched by the regex
            for match_name in fuzzy_match.groupdict():
                # We stick into the holder
                # a slice of the input seq, that is the matched
                # span of this matching group
                seq_holder[match_name] = \
                    seq_holder[operation[0]]\
                    [slice(*fuzzy_match.span(match_name))]
                # Then we record the start, end, and length of the
                # matched span, again only if we have filters
                if filters is not None:
                    (scores_holder[match_name+'_start'],
                        scores_holder[match_name+'_end']
                        ) = fuzzy_match.span(match_name)
                    scores_holder[match_name+'_length'] = \
                        (scores_holder[match_name+'_end'] - 
                            scores_holder[match_name+'_start'])


    # All these values allow use to apply filters, using this
    # function
    evaluated_filters = evaluate_filters(filters, {**scores_holder, **seq_holder} )

    # This evaluated_filters should be logical list. So did we pass all filters?
    if not all(evaluated_filters):

        if verbosity >= 2:
            print("\n["+str(time.time())+"] : match is : evaluated the "+
                "filters as : "+str(evaluated_filters)+" and so failed.", 
                file=sys.stderr)

        # So if we should write this per-record report
        if report is not None:
            print("\"FailedFilter\",\""+
                    str(input_record.seq)+"\",\""+
                    str(evaluated_filters)+"\",\""+
                    str(input_record.id)+"\",\""+
                    str(input_record.seq)+"\",\""+
                    re.sub("\"","\\\"",re.sub(",","\,",
                        json.dumps(scores_holder)))+"\""
                    ,
                file=report)
        # If this json dump is empty, it might be because it didn't
        # ever match the first operation, so then just died without
        # building that object

        if failed is not None:
            SeqIO.write(input_record, failed, "fastq")

        return 0

    else:

        try:
            # We attempt to form the correct output records
            output_records = [ evaluate_output_directives(i, j, seq_holder) for i, j in outputs_array ]
            # So this will fail us out of the 'try' if it doesn't form

            # Otherwise, just the record and if it passed
            if out_format == "sam":
                for which, output_record in enumerate(output_records):
                    print(
                        "\t".join([
                            output_record.id,
                            "0", "*", "0", "255", "*", "=", "0", "0", 
                            str(output_record.seq),
                            ''.join([chr(i+33) for i in output_record.letter_annotations['phred_quality']]),
                            "XI:"+str(which)
                            ])
                        ,file=output)
            elif out_format == "fastq":
                for output_record in output_records:
                    SeqIO.write(output_record, output, "fastq") 
            else:
                print("I don't "+out_format+" format, looping over here") 

            if verbosity >= 2:
                print("\n["+str(time.time())+"] : evaluated the "+
                    "filters as : "+str(evaluated_filters)+" and so passed.", 
                    file=sys.stderr)

            # If we want to write the report, we make it
            if report is not None:
                [ print("\"Passed\",\""+
                        str(output_record.seq)+"\",\""+
                        str(evaluated_filters)+"\",\""+
                        str(input_record.id)+"\",\""+
                        str(input_record.seq)+"\",\""+
                        re.sub("\"","\\\"",re.sub(",","\,",
                            json.dumps(scores_holder)))+"\""
                        ,
                    file=report)
                        for output_record in output_records ]

            return 0

        except:

            if verbosity >= 2:
                print("\n["+str(time.time())+"] : failed upon forming the "+
                    "output.", file=sys.stderr)

            # If we want to write the report, we make it
            if report is not None:
                print("\"FailedDirectivesToMakeOutputSeq\",\""+
                        str(input_record.seq)+"\",\""+
                        str(evaluated_filters)+"\",\""+
                        str(input_record.id)+"\",\""+
                        str(input_record.seq)+"\",\""+
                        re.sub("\"","\\\"",re.sub(",","\,",
                            json.dumps(scores_holder)))+"\""
                        ,
                    file=report)

            if failed is not None:
                SeqIO.write(input_record, failed, "fastq")

            return 0

def evaluate_output_directives(output_id, output_seq, seq_holder):
    # Here we evaluate them but using that dictionary as the global
    # dictionary, because done is better than dogma.
    return_record    = eval(output_seq,{},seq_holder)
    return_record.id = eval(output_id, {},seq_holder)
    return(return_record)


def evaluate_filters(filters,holder):
    return_object = []
    try:
        for each_filter in filters:
            # Here we evaluate them but using that dictionary as the
            # global dictionary, because done is better than dogma.
            if eval(each_filter,globals(),holder):
                return_object.append(True)
            else:
                return([False])
    except:
        return([False])
    return(return_object)


if __name__ == '__main__':
    # This `if` statement only runs if it's called alone, I believe. So I think 
    # you can import these functions for use in other scripts, but I'm not 
    # intending for that. This is supposed to be standalone.

    # Using argparse module to define the arguments

    # Name and description of this program
    parser = argparse.ArgumentParser(description=""+
        "itermae - Tool for iteratively chopping up each FASTQ read using "+
        "fuzzy regular expressions, for many many reads."+
        "\n\n"+
        "For best performance, we recommend you use this with GNU parallel, "+
        "see examples in the README.")

    # Input determination 
    parser.add_argument("--input",default="STDIN",
        help="We expect this is normally NOT USED, and so I will expect "+
            "FASTQ(Z) input on the STDIN. However, you can specify a path "+
            "using this flag to read from a file. This is not parallelized, "+
            "so it's slower. But it's there.")
    # Is it gzip'd ?
    parser.add_argument("-z","--gzipped",action="store_true",
        help="This flag indicates that input, be it STDIN or a file path, "+
            "is GZIPPED FASTQ. Note that this implies that any STDIN usage is "+
            "not parallel, so we do not expect this to be used in combination "+
            "with the STDIN input. But it's there.")

    # Output determination
    parser.add_argument("--output",default="STDOUT",
        help="we expect this flag is normally NOT USED, and so output would "+
            "be send to STDOUT. However, it's here incase you'd like to write "+
            "it to a file. Note I do not add a suffix for you.")
    parser.add_argument("--output-format",default='sam',
        help="The output format specification. Default is an unmapped sam "+
            "('sam') so that the tabular nature can permit joining with other "+
            "parsed output from the reverse read, but you can also specify "+
            "'fastq' to get a FASTQ back.")
    parser.add_argument("--failed",default=None,
        help="Optional name of output file for failed reads, for debugging. "+
            "If you say 'STDOUT' then it'll go there, so you could use that "+
            "to collect failed and passed reads in the same 'sam' file for "+
            "example.")
    parser.add_argument("--report",default=None,
        help="Add this flag with a filename to print a report of per-read "+
            "statistics. That's a lot of disk writes btw, but "+
            "would be good in combination with a '--limit' argument "+
            "so that you can spec out the kinds of noise you got "+
            "in your data and debug the running.")

    # verbosity
    parser.add_argument("-v","--verbose",action="count",default=0,
        help="How much debugging issues should I pipe out to STDERR?"+
            "None is nothing, "+
            "-v is setup messages and start-stop messsages, "+
            "-v -v is worker-level details, "+
            "-v -v -v is chop-level details, "+
            "-v -v -v -v is each operation level details. "+
            "Keep in mind these are going to STDERR.")

    ### CLI operation and output specification
    # Operations
    parser.add_argument("--operation","-o",action="append",
        help="The pattern to match and extract. This has a "+
            "specific and sort of complicated syntax. Refer to "+
            "the documentation via the README.md file, or to "+
            "the original regex module documentation. "+
            "They are chained in the order you specify. "+
            "You can't name a group 'input' or 'dummyspacer', I'm using those "+
            "names !")
    # Filter specification
    parser.add_argument("--filter","-f",action="append",
        help="Filters for eliminating reads that don't pass some criteria. "+
            "You write these based on the groups captured in the operations "+
            "above. So a group named 'group' would have a start at "+
            "'group_start', same for the 'group_end' and 'group_length'. "+
            "You're welcome. Also, `statistics` package is loaded, so you "+
            "can use those expressions for means, medians, etc.")
    # Outputs
    parser.add_argument("--output-id",action="append",
        help="A list of output ID definitions, in the same order as for "+
            "output-seq (see that one, below probably). This is evaluated for "+
            "reads that pass filter. You can access 'input.id' to get the "+
            "original FASTQ read ID, and 'group.seq' to get the sequence of a "+
            "particular group, so for example you could do "+
            "'input.id+\"_\"+index.seq' to append the index sequence to the "+
            "FASTQ ID.")
    parser.add_argument("--output-seq",action="append",
        help="A list of output seq definitions, in the same order as for "+
            "output-ids. This is evaluated on the groups being Biopython "+
            "SeqRecords, so you need to just specify the names of the "+
            "captured groups. You can't access .id or _length properties or "+
            "the like! This is not for that, put it in the "+
            "output-id. Why? Because we need the qualities for printing.")

    ### Parse the arguments, check them.

    args = parser.parse_args()

    # Operations, outputs are read as an array of dicts to keep it ordered.
    operations_array = []
    outputs_array = []
    # Here we read on through to add those on, and complain loudly if someone
    # tries to use our reserved names.
    try:
        for each in args.operation:
            if each.find("<dummyspacer>") > 0:
                print("Hey, you can't name a capture group "+
                    "'dummyspacer', I'm using that! Pick a different name."
                    ,file=sys.stderr)
                exit(1)

            if each.find("<input>") > 0:
                print("Hey, you can't name a capture group "+
                    "'input', I'm using that! Pick a different name."
                    ,file=sys.stderr)
                exit(1)
            # Here, we use the ` > ` to specify the flow of input to
            # the regex
            (input_string, regex_string) = re.split("\s>\s",each.strip())
            compiled_regex = regex.compile(
                regex_string.strip(), # We use this regex
                regex.BESTMATCH # And we use the BESTMATCH strategy, I think
                )
            # append that to the operations array
            operations_array.append( [input_string.strip(), compiled_regex] )
    except:
        # Failure likely from lack of operations to do
        print("Wait a second, I don't understand the operations to be done! "+
            "Are there any? Maybe there's small part I'm choking on? Maybe "+
            "try adding steps in one at a time in an interactive context with "+
            "'--limit' set, to debug easier. Exiting...",file=sys.stderr)
        exit(1)

    # Next we build the array of outputs, by combining the named output IDs
    # and seq specifications.
    try:
        for each_id, each_seq in zip(args.output_id, args.output_seq):
            # append that to the outputs array
            outputs_array.append( [each_id, each_seq] )
    except:
        # Failure likely from lack of operations to do
        print("Wait a second, I don't understand the outputs to be done! "+
            "Are there any? Maybe there's small part I'm choking on? Maybe "+
            "try adding steps in one at a time in an interactive context with "+
            "'--limit' set, to debug easier. Exiting...",file=sys.stderr)
        exit(1)

    if args.verbose >= 1:
        print("\n["+str(time.time())+"] : "+
            "I'm reading in something, applying these operations of "+
            "alignment:\n",file=sys.stderr)
        for each in operations_array:
            print("  - from : "+each[0]+"\n"+
                "    extract groups with regex : '"+str(each[1])
                ,file=sys.stderr)

    if args.verbose >= 1:
        print("\n["+str(time.time())+"] : ...and with these filters:\n",
            file=sys.stderr)
        try:
            for i in args.filter:
                print("  - "+i,file=sys.stderr)
        except:
            print("  ( no filters defined )",file=sys.stderr)

    if args.verbose >= 1:
        print("\n["+str(time.time())+"] : "+
            "Then I'm going to construct outputs that look like:\n",
            file=sys.stderr)
        for each in outputs_array:
            print("  - With ID of : "+each[0]+"\n"+
                "    and the sequence is the group(s) : "+each[1],file=sys.stderr)

    # If it's omitted, we believe that means no filter, and we make it True
    # because it gets `eval`'d in the function. 
    if args.filter is None:
        args.filter = ["True == True"]

    if args.verbose >= 1:
        print("\n["+str(time.time())+"] : Then, I'm going to write out a "+
            args.output_format+" format file to "+
            args.output+"",file=sys.stderr)
        if args.report is not None:
            print("\n["+str(time.time())+"] : and a report to '"+
                vars(args)["report"]+".",file=sys.stderr)

#    # checking file existance for outputs, zipping together the 
#    # output base with each of the three. 
#    exit_flag = 0
#    for each in zip( [vars(args)["output-base"]]*20,            \
#                    ["_fail.fastq", "_pass.fastq",              \
#                        "_report.fastq", "_report.csv" ] ):
#        # At this stage, the tuple is joined to make the filename
#        this_path = ''.join(each)
#        # If the write-report flag is off and the path is the report,
#        # then this won't trip True for that path existing
#import os.path
#        if os.path.isfile(this_path) and              \
#                not( not(args.write_report) and       \
#                    (this_path.find("_report")>=0) ):
#            print("\n"+"["+str(time.time())+"]"+" : "+"File "+this_path+
#                " exits, so I'm quitting before you ask me to do "+
#                "something you might regret.")
#            exit_flag = 1
#    if exit_flag == 1:
#        exit(1)

#    # memory debugging
#    if args.memory_tracking:
#        tracemalloc.start(10)
#import tracemalloc

    # We begin
    if args.verbose >= 1:
        print("\n["+str(time.time())+"] : BEGIN RUNNING",file=sys.stderr)
    
    reader(
        vars(args)["input"],
        args.gzipped,
        vars(args)["output"],
        vars(args)["failed"],
        vars(args)["report"],
       # args.memory_tracking,
        operations_array, 
        args.filter ,
        outputs_array,
        args.output_format,
        args.verbose
        )

    print("\n"+"["+str(time.time())+"]"+" : "+
        "All worked 'till the work is done --- or some fatal error.",file=sys.stderr)

    exit(0)
